For this homework, I worked with AI being able to classify different styles. I had it generate paragraphs in different styles, and then ran the paragraphs back into the model and asked it to classify and rate the styles.

It took a long time for me to figure out how to do the structured output formally. I first thought I need a field called response_format, but the client said that wasn't a function parameter, so I had to implement Pydantic. One thing I tried was to give instructions and structure that contradicted each other. I told the structure to include reasoning, but the instructions to not, and it listened to the prompt and just left the reason field blank. Similarly, when I ommitted a reason field from the structure, but said to include it in the prompt, it snuck the reasoning into the style field. As I tried to get it to classify the paragraphs, I kept running into a problem where it would only classify the first paragraph. It turned out that the problem was my structured output - I was only letting it return one object, not multiple. To fix this, I had to wrap my new object type in another type that would format my classifications as a list. One thing I liked about the structured output was that the agent didn't yap too much about it.

Prompts used (Part 1): You will be given input of paragraphs written in different styles. For each paragraph, you need to classify the style of the paragraph (Shakespeare, Cowboy, etc), give it a score from 1-10 of how well it used that style, and then give your reasoning for that score.

I also played around with giving the AI different roles to help with the prompt. I noticed that it REALLY doesn't like to be mean, and would rate everything at like an 8 or 9 out of 10 for the longest time. It wasn't until I told it to be a TA peer-reviewing the paragraphs, and that being mean would actually be more helpful to the 'student'.

I also ran it with less specific prompts, that didn't give super clear descriptions about what I wanted. When I didn't have as much detail, I noticed that the same prompt could produce really big differences in the output formatting, based on what it thought it should include from some of the details I had in the role assignment. I then gave it instructions to order the paragraphs from best to worst. When I left it vague, I expected it to produce different orderings, but it was mostly constant. However, I was impressed that it inferred what I meant by "best" (as in the most stylistically accurate) just from a little bit of information about being an English TA, and grading paragraphs in different styles.

At first, I ran it on a smaller version of my stylistic paragraphs, taking one from each group of 5 paragraphs. Then, I ran it on a larger file which had 5 paragraphs in each style. The first time I ran it, it was able to split it up totally fine, but the next time it combined sections together, and eventually said it had a hard time separating it without breaks in the input. I found that it was actually more picky when I tried to give it specific instructions (11 sections, each 5-6 paragraphs long), but did better when I gave it a guideline for splitting up the information. I also found that it mattered where I gave the advice for splitting up the information.