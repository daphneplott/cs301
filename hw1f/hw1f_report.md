One thing I really liked about Elder Bednar’s message was how he emphasized that AI shouldn’t take away our ability to create. My mom is a high school teacher, so one way that AI is present in my life is how much she complains about her students using it. They rely on AI to help with their homework, and their essays, and they don’t ever bother learning their school materials for themselves. They don’t have any pride in their work and what they are doing, they just pass it on to AI to do it for them. I think that runs a really big danger of sacrificing our creative ability. I think that one thing that will help us fight AI is just learning to care about doing things ourselves, and finding joy in what we do. I think that if we learn to like the process itself, we’d be less interested in giving some of that to AI to fix or to solve. We shouldn’t let AI take away our ability to do something new. When it comes down to it, the only thing AI really can’t do, is think of something new, which is the best part of what humanity can do. 

Another important piece of Elder Bednar’s message is that AI doesn’t feel anything. I think that sometimes we treat AI as if it’s like the robots in the movies, which do sort of have emotions. I think that I especially need to remember that when I interact with some of these agents, although they may pretend to have emotions, and although they may eventually progress to where they sort of can actually think and have emotions,I need to treat them like computers and algorithms. They’re currently more like video game characters than like C-3PO or R2-D2. We should be careful not to personify AI too much, or we risk forgetting what it really is, and what its limitations are.

I think the most important thing for us to have in mind as we continue to live in a world with AI is to not be afraid to take a step back from a situation and ask “Is this real?” I think it’s easy to just believe in things, it’s easy to not think too hard, or to just trust information. But because we can have the influence of the Holy Ghost, we need to give it space to work. Before jumping into anything, we should think about it, and reflect about how we’re feeling about it.

As I read online about recent articles, so many of them were about the bad things that AI had done. One article was about a support group that was being formed to help people who had been sucked in to extended hallucinations, and then were caught in an agreement loop with AI until they sort of went crazy. Besides from the scary parts there, I really liked how the article emphasized that the support group was helping because of human connections, and how human connections have friction. I think this is really important to remember. Although we may not like disagreement, it's an important part of life. As an engineer, part of my job is to set boundaries on agents so they can't get sucked into these hallucination spirals. Yes, AI may hallucinate, but I need to make sure not to facillitate extended hallucinations. I also need to make sure that the people who interact with AI really do understand how un-real they are. 

Another interesting story I real was about Dr Google vs Dr ChatGPT. This article was trying to decide if ChatGPT was an effective medical assistant. What they found was that ChatGPT was mostly correct, and was even comparable to licensed professionals. As much as we try to avoid situations like this, as long as the internet is around, people will use Dr Google, so it begs the question of, if Chat is better, maybe people really should be using it. However, I think that most professionals would argue that using a real doctor is better in all cases, so maybe making services like Dr ChatGPT is more harmful in the long term.

When it comes to appropriate uses for AI, our decisions should be primarily based on the fact that it is not human, and it's not even a movie-esque futuristic robot. We can't confuse it with something that is intelligent, however realistic it plays that role. I've always sort of considered that advanced human intelligence doesn't just come from our brains and nuerons, but that deep down our thinking processes are indicative of our spirits. Although AI can simulate the neurons, it can't simulate that. So, AI shouldn't be used for anything that has it simulate emotions or real people. It shouldn't be used to replace genuine human creativity. It shouldn't be used as a shortcut for learning. It shouldn't be used to simulate licensed professionals. However, I think that it's appropriate to use AI as an upgraded Google search tool. It can be used to help explain topics, but shouldn't be used as a give-me-an-answer tool. It can be used to help automate tasks, or even generate recommendations for future actions, but I think there should be humans in charge of making any meaningful decisions. I also think that when a person is using a chatbot, it needs to be 100% clear, even slap-you-over-the-head clear that you're not talking to a real person.