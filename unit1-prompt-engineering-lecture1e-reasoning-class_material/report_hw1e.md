1) Try reasoning models on basic tasksâ€”where does it work well? Where not?
    - What kinds of problems benefit from reasoning?
    - Are there problems where it doesn't seem to make a difference?
2) Quantify how much more expensive (in time and tokens) are responses with reasoning enabled?
    - If someone asked you how much longer it would take, or how much more it would cost, do you have an intuition for that?

- solving math problems (i.e. Elementary vs Junior High vs High School vs College)
- coding or logic puzzles
- cipher text
- games (i.e. 20 questions, chess)
- stacking physical objects (i.e. "Here we have a book, 9 eggs, a laptop, a bottle and a nail. Please tell me how to stack them onto each other in a stable manner.")
- planning dinner (i.e. "I have eggs, spinach, and mushrooms in my fridge. What recipes can I make for dinner?")
- giving vague instructions (i.e. "make this better")
- asking for simple facts (i.e. "Is this chair stable?", "Who is the president of the United States?")
- asking philosophy questions


## Parts 1 & 2
For these exercises, I asked the same questions to two agents, one with reasoning enabled, and one without reasoning enabled.

## Part 3

One thing I thought was really interesting about the constitution was how they arranged it to help 'teach' Claude good behaviour and ethics. They were hoping to make it so that it wasn't just a set of rules, but that the model would internalize the lessons to better apply the information. I was also suprised because they talked quite a lot about moral questions of who or what is Claude? They included a lot of notes about sense of identity, mental stability, having values, and expressing emotions. It's really interesting to see the way Anthropic thinks about their AI agent. We always say "well, it's not real", but it makes me wonder just exactly how real and mentally present the model is.