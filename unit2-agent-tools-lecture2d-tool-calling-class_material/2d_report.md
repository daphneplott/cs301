I first experimented with random number generation. I used the python random library to generate integers between 1 and 100, and then graphed a distribution of those numbers. I did this a few times, and the distributions seemed fairly even (especially when I generated more an more random numbers). When I tried to do this with AI, I felt like it was actually fairly well distributed between 1 and 100. However, I ended up with an issue of it not listening right to the instructions. I told it to generate 1000 numbers between 1 and 100, but it returned over 3000, and had a few above 100. Using the built in function really helped with the consistency. I knew I could trust the output from that, but wasn't as certain with the LLM repsonse.

Next, I had copilot write a web tool that fetches a given webpage. I wrote a system prompt for the quote finder that explained what the agent was supposed to do, and outlined the steps for it. One initial problem I ran into was that I gave the model the wrong page to start at initially. The first url I gave it didn't include every speaker's page, so it ended up guessing on what it thought the url was, but because I didn't include the middle name, it guessed the wrong page. The next error I ran into was that the input (which included the webpages), exceeded the context window of my given model. One way I fixed this was to just give the agent the page to the most recent conference. This helped, but didn't fix the root of the problem. I was trying to delete some of the history, but the way that the openai api is set up to expect tool calls kept causing it to throw an error if I had deleted the tool response but not the call message in the history. The solution I ended up with (with help from Chat) was to not ever put the entire first web page in, but use a different call to get the agent to just pull out the singular link I needed, and then return that as the tool-call response, which helped fix the problem. I combined the two functionalities into the same tool, and at first the agent had a hard time knowing when to get just the link, and when to return the entire page. One thing that made a difference was in changing the auto value of the 'just_link' parameter from False to True.

When I gave the agent just the link to the most recent conference, it did very well at finding the talk. However, when I gave it the speaker index page, it had a hard time deciding on the next step. It would retrieve the speaker's page, but then not retrieve the pages that held the talks to look through those. I think this happened because I didn't give it specific instructions to look through the talks, but my prompt implied that the page it got would have the right information on it. Part of the issue was also in how I wrote the docstring for the web tool. I specified that it should not return the entire web page unless it was absolutely necessary. I had written this so that it would actually use the just_link functionality of the tool, but it ended up making the agent very reluctant to actually get the information, and I had to prompt it a lot to actually take the actions. But when I removed that note, it just went straight ahead to search through the webpages.

One thing I noticed was that this program was very costly. There were a lot of input tokens because I was returning the entire page, and this drove the cost up quite a bit. I think that using RAG is a cheaper way to do this, combined with maybe a python web crawler. Maybe another way to reduce the cost would be to have some sort of python pre-processor that could get rid of the unhelpful webpage information, and just return the meat of the page.