The first thing I played around with was the web search tool. The first thing I did was to restrict sites to only byu, and then ask the model about the superbowl. One thing I tried to do was to make sure it didn't tell the user what sites it was restricted to. To do this, in the system prompt I included the note that if it didn't find what it was looking for, it should say "I'm sorry, I can't help with that", and end the chat. One thing I noticed as I refined the prompt was that including quotation marks caused to model to put out that exactly. Prompting it to end the chat also helped this. The web search tool also has a user location feature you can use. This helps the model add into their search something that we do naturally when using google - location detection. This allows me to ask questions like 'what is the weather' and it will infer where I am. It also helped with a question of 'how are the cougars?'. When I told it I was in London, it gave me a list of options, such as the animals, or sports teams. When I told it I was in Provo, it jumped immediately to BYU cougars, even though it still had follow up questions about which sports, and if that's what I actually meant.

Next I experimented with the run python tool. At first, I started with the human-in-the-loop code provided in the class material. The first question I posed to it was a logic puzzle about a knight moving in a square, and spelling out birds. One issue I ran into was false assumptions. It made assumptions about what these birds would be, but missed something, so the code kept failing. Then, whatever code it wrote would return wrong answers for some reason. It was almost too hard to look through what all the code was to figure out what it was doing. One thing that helped was giving it a better prompt to fix that false assumption, and it did better after that. 

One question I presented was with an adding letters to existing words to construct new words. One problem I ran into was that the model wasn't very good at recognizing English words. So when it was trying to run a brute force algorithms, it first wanted to import some sort of dictionary. I told it to just look through all the options itself, but it didn't end up returning everything, like it didn't want to fill the context window too much. I felt like I had to coach it through figuring out some of these word problems. Even when I had it return everything, it had a hard time picking out the permutations that were real words. One thing that eventually helped was asking it for some of the top possibilites. I worked with a few other logic puzzles that can be solved with brute force computations. I found that the model did better when there was some quality that it could make the python script check through, something more numerical based.

I next asked it a couple questions that relate to algorithms from 312. I first asked for shortest paths on a small graph. At first, it just did it itself because the graph was small, but when I asked it to verify, that's when it wrote Dijkstra's. It was able to figure out scc's without needing to write code. I asked it to generate RSA keypairs as well, and it did well with that. One thing that was interesting with that was that it rewrote the script a few times. I think that it was potentially getting an error message, like a bad import statement, that the python script didn't print out for me.

One thing I didn't like about the given run python tool was the response it gave when I indicated to not run it. What I wanted it to do (but didn't bother changing until the very end) was have the LLM ask me what was wrong with it, and why I didn't want it to run it. I think that would be a better message to send back from the tool. Even when I did change it, the model didn't ask what was wrong as clearly as I hoped it would. I wonder if this shows that the algorithm places more importance on information from certain sources than from others. I think this is something I could fix with a better system prompt.