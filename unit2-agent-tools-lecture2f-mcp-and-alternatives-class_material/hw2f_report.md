I spent some time looking at the mcp tool calling. I looked through the stock server code, to see how the mcp works. I was able to call the server itself, but I didn't install ngrok to link it to my agent. One thing that was interesting was that I asked Chat about the error message it returned. I knew it would be the local host thing, but Chat initially didn't pick up on that. After that, I looked at running the Google Maps mcp. I tried to get Chat to help me figure out what to add into the tools, but it really struggled to figure out how to just put the google server information in like we did in class. I didn't want to register an api key with google maps for just a single use (and I'd spent most of my time already reading materials for next week), so I tried to call the tool without giving it the api key. I was expecting it to crash, but it just said it couldn't run a live search. I used the demo code from class to work with this.

What I spent most of my time on was reading through the materials for next class. I thought it was really interesting how many bad behavior issues they ran into. I feel like one of the main problems we might have in the future is with how ai percieves itself. If it starts going rouge, or feels like it has to defend itself, that's when we're going to get into our science fiction issues. I think it also shows that we're not ready to let AI be agentic yet. We can't let it make decisions all on its own, because we have no idea what it's going to do. As much as it would be nice to be easy, we still have a lot of work to do. We should also be careful about giving AI access to our private data, because it can apparently go too far. Along with the required article, I also read part of one that was posted in the discord, which was also from Anthropic, talking about the potentially scary consequences of AI. One thing I liked from that article was how it said that we shouldn't necessarily be panicking yet, or deciding it's the robot apocalypse, but we do need to be thinking about it. We need to be careful about who is designing ai systems, and how. The underlying personality of the models can drastically affect some of these situations.

One thing I liked about the other assigned article was the possibility it gave that we can train AI to have better morals. In this society, especially, I think that we as humans aren't modeling the behavior we want our agents to have. It clearly learned about all these issues from somewhere, so we have to figure out how to train that out of our agents. I think with how society is right now, though, that's going to end up being very difficult. AI doesn't have the spirit of discernment, all it has is the internet. Relating back to the Claude constitution we read earlier, I think this is why they tried so hard with it. If AI is acting in very human like manners, then maybe they need something more general to teach AI how to act. 